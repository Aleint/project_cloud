ansible-playbook [core 2.17.9]
  config file = /etc/ansible/ansible.cfg
  configured module search path = ['/home/ale/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']
  ansible python module location = /usr/lib/python3/dist-packages/ansible
  ansible collection location = /home/ale/.ansible/collections:/usr/share/ansible/collections
  executable location = /usr/bin/ansible-playbook
  python version = 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (/usr/bin/python3)
  jinja version = 3.0.3
  libyaml = True
Using /etc/ansible/ansible.cfg as config file
host_list declined parsing /home/ale/Desktop/edge-infrastructure-ansible/inventory as it did not pass its verify_file() method
script declined parsing /home/ale/Desktop/edge-infrastructure-ansible/inventory as it did not pass its verify_file() method
auto declined parsing /home/ale/Desktop/edge-infrastructure-ansible/inventory as it did not pass its verify_file() method
Parsed /home/ale/Desktop/edge-infrastructure-ansible/inventory inventory source with ini plugin
statically imported: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/nginx_installation/tasks/nginx_ingressController.yaml
statically imported: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/prometheus-grafana.yaml
statically imported: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/kepler.yaml
redirecting (type: modules) community.kubernetes.k8s to kubernetes.core.k8s
redirecting (type: action) community.kubernetes.k8s to kubernetes.core.k8s_info
redirecting (type: modules) community.kubernetes.k8s to kubernetes.core.k8s
redirecting (type: action) community.kubernetes.k8s to kubernetes.core.k8s_info
Skipping callback 'default', as we already have a stdout callback.
Skipping callback 'minimal', as we already have a stdout callback.
Skipping callback 'oneline', as we already have a stdout callback.

PLAYBOOK: env_setup.yaml *******************************************************
2 plays in playbook/env_setup.yaml

PLAY [Cluster prep] ************************************************************

TASK [Gathering Facts] *********************************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/env_setup.yaml:11
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865075.4582152-24334-155312874108840 `" && echo ansible-tmp-1741865075.4582152-24334-155312874108840="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865075.4582152-24334-155312874108840 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/setup.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmpymfnoy3r TO /home/ale/.ansible/tmp/ansible-tmp-1741865075.4582152-24334-155312874108840/AnsiballZ_setup.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865075.4582152-24334-155312874108840/ /home/ale/.ansible/tmp/ansible-tmp-1741865075.4582152-24334-155312874108840/AnsiballZ_setup.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-mtcdmwvhlqwugyzzfxlnazwzarjirgqj ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865075.4582152-24334-155312874108840/AnsiballZ_setup.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865075.4582152-24334-155312874108840/ > /dev/null 2>&1 && sleep 0'
ok: [localhost]

TASK [Get OS language] *********************************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/env_setup.yaml:19
skipping: [localhost] => {
    "false_condition": "not ansible_env.LANG.startswith('en')"
}

TASK [Get k3s version] *********************************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/env_setup.yaml:24
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865077.38749-24467-35624701852546 `" && echo ansible-tmp-1741865077.38749-24467-35624701852546="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865077.38749-24467-35624701852546 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmp6yg_927h TO /home/ale/.ansible/tmp/ansible-tmp-1741865077.38749-24467-35624701852546/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865077.38749-24467-35624701852546/ /home/ale/.ansible/tmp/ansible-tmp-1741865077.38749-24467-35624701852546/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-vaowqvimormqjirmsvqsnacqhxwcihfh ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865077.38749-24467-35624701852546/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865077.38749-24467-35624701852546/ > /dev/null 2>&1 && sleep 0'
ok: [localhost] => {
    "changed": false,
    "cmd": [
        "k3s",
        "--version"
    ],
    "delta": "0:00:00.008337",
    "end": "2025-03-13 12:24:37.753288",
    "invocation": {
        "module_args": {
            "_raw_params": "k3s --version",
            "_uses_shell": false,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "",
    "rc": 0,
    "start": "2025-03-13 12:24:37.744951",
    "stderr": "",
    "stderr_lines": [],
    "stdout": "k3s version v1.31.6+k3s1 (6ab750f9)\ngo version go1.22.12",
    "stdout_lines": [
        "k3s version v1.31.6+k3s1 (6ab750f9)",
        "go version go1.22.12"
    ]
}

TASK [Get kubectl version] *****************************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/env_setup.yaml:30
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865077.8148775-24500-267909556134161 `" && echo ansible-tmp-1741865077.8148775-24500-267909556134161="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865077.8148775-24500-267909556134161 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmpraium_f3 TO /home/ale/.ansible/tmp/ansible-tmp-1741865077.8148775-24500-267909556134161/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865077.8148775-24500-267909556134161/ /home/ale/.ansible/tmp/ansible-tmp-1741865077.8148775-24500-267909556134161/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-lgbcizeqbktflijjzfdjzneezympcgoo ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865077.8148775-24500-267909556134161/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865077.8148775-24500-267909556134161/ > /dev/null 2>&1 && sleep 0'
fatal: [localhost]: FAILED! => {
    "changed": false,
    "cmd": [
        "kubectl",
        "version",
        "--client",
        "--short"
    ],
    "delta": "0:00:00.191277",
    "end": "2025-03-13 12:24:38.224817",
    "invocation": {
        "module_args": {
            "_raw_params": "kubectl version --client --short",
            "_uses_shell": false,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "non-zero return code",
    "rc": 1,
    "start": "2025-03-13 12:24:38.033540",
    "stderr": "error: unknown flag: --short\nSee 'kubectl version --help' for usage.",
    "stderr_lines": [
        "error: unknown flag: --short",
        "See 'kubectl version --help' for usage."
    ],
    "stdout": "",
    "stdout_lines": []
}
...ignoring

TASK [Get minikube version] ****************************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/env_setup.yaml:36
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865078.2870507-24541-166793293537018 `" && echo ansible-tmp-1741865078.2870507-24541-166793293537018="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865078.2870507-24541-166793293537018 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmp0pwj675g TO /home/ale/.ansible/tmp/ansible-tmp-1741865078.2870507-24541-166793293537018/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865078.2870507-24541-166793293537018/ /home/ale/.ansible/tmp/ansible-tmp-1741865078.2870507-24541-166793293537018/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-cycqzjcbgmeggpinelhebvkibqujfyrn ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865078.2870507-24541-166793293537018/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865078.2870507-24541-166793293537018/ > /dev/null 2>&1 && sleep 0'
The full traceback is:
  File "/tmp/ansible_ansible.legacy.command_payload_7vr7ksw4/ansible_ansible.legacy.command_payload.zip/ansible/module_utils/basic.py", line 1933, in run_command
    cmd = subprocess.Popen(args, **kwargs)
  File "/usr/lib/python3.10/subprocess.py", line 971, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/usr/lib/python3.10/subprocess.py", line 1863, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
fatal: [localhost]: FAILED! => {
    "changed": false,
    "cmd": "minikube version",
    "invocation": {
        "module_args": {
            "_raw_params": "minikube version",
            "_uses_shell": false,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "[Errno 2] No such file or directory: b'minikube'",
    "rc": 2,
    "stderr": "",
    "stderr_lines": [],
    "stdout": "",
    "stdout_lines": []
}
...ignoring

TASK [Get MicroK8s version] ****************************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/env_setup.yaml:42
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865078.6077747-24569-135605441732170 `" && echo ansible-tmp-1741865078.6077747-24569-135605441732170="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865078.6077747-24569-135605441732170 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmpnlippra2 TO /home/ale/.ansible/tmp/ansible-tmp-1741865078.6077747-24569-135605441732170/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865078.6077747-24569-135605441732170/ /home/ale/.ansible/tmp/ansible-tmp-1741865078.6077747-24569-135605441732170/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-nwkceejirocawpnlttscglbmuryahmnh ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865078.6077747-24569-135605441732170/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865078.6077747-24569-135605441732170/ > /dev/null 2>&1 && sleep 0'
The full traceback is:
  File "/tmp/ansible_ansible.legacy.command_payload_ivi68__8/ansible_ansible.legacy.command_payload.zip/ansible/module_utils/basic.py", line 1933, in run_command
    cmd = subprocess.Popen(args, **kwargs)
  File "/usr/lib/python3.10/subprocess.py", line 971, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/usr/lib/python3.10/subprocess.py", line 1863, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
fatal: [localhost]: FAILED! => {
    "changed": false,
    "cmd": "microk8s version",
    "invocation": {
        "module_args": {
            "_raw_params": "microk8s version",
            "_uses_shell": false,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "[Errno 2] No such file or directory: b'microk8s'",
    "rc": 2,
    "stderr": "",
    "stderr_lines": [],
    "stdout": "",
    "stdout_lines": []
}
...ignoring

TASK [Get OpenShift (oc) version] **********************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/env_setup.yaml:48
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865078.921663-24597-132587853387226 `" && echo ansible-tmp-1741865078.921663-24597-132587853387226="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865078.921663-24597-132587853387226 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmph04cbvrl TO /home/ale/.ansible/tmp/ansible-tmp-1741865078.921663-24597-132587853387226/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865078.921663-24597-132587853387226/ /home/ale/.ansible/tmp/ansible-tmp-1741865078.921663-24597-132587853387226/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-fbcbrllxgheijoouqetnwcjzwxnepkjd ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865078.921663-24597-132587853387226/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865078.921663-24597-132587853387226/ > /dev/null 2>&1 && sleep 0'
The full traceback is:
  File "/tmp/ansible_ansible.legacy.command_payload_e1ag1xq1/ansible_ansible.legacy.command_payload.zip/ansible/module_utils/basic.py", line 1933, in run_command
    cmd = subprocess.Popen(args, **kwargs)
  File "/usr/lib/python3.10/subprocess.py", line 971, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/usr/lib/python3.10/subprocess.py", line 1863, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
fatal: [localhost]: FAILED! => {
    "changed": false,
    "cmd": "oc version",
    "invocation": {
        "module_args": {
            "_raw_params": "oc version",
            "_uses_shell": false,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "[Errno 2] No such file or directory: b'oc'",
    "rc": 2,
    "stderr": "",
    "stderr_lines": [],
    "stdout": "",
    "stdout_lines": []
}
...ignoring

TASK [Get helm version] ********************************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/env_setup.yaml:54
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865079.208072-24625-206300025361213 `" && echo ansible-tmp-1741865079.208072-24625-206300025361213="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865079.208072-24625-206300025361213 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmppdrm732n TO /home/ale/.ansible/tmp/ansible-tmp-1741865079.208072-24625-206300025361213/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865079.208072-24625-206300025361213/ /home/ale/.ansible/tmp/ansible-tmp-1741865079.208072-24625-206300025361213/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-fyaeppwhcdhpjsngcbzxgurrrlnvackn ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865079.208072-24625-206300025361213/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865079.208072-24625-206300025361213/ > /dev/null 2>&1 && sleep 0'
ok: [localhost] => {
    "changed": false,
    "cmd": [
        "helm",
        "version"
    ],
    "delta": "0:00:00.070537",
    "end": "2025-03-13 12:24:39.512360",
    "invocation": {
        "module_args": {
            "_raw_params": "helm version",
            "_uses_shell": false,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "",
    "rc": 0,
    "start": "2025-03-13 12:24:39.441823",
    "stderr": "",
    "stderr_lines": [],
    "stdout": "version.BuildInfo{Version:\"v3.17.1\", GitCommit:\"980d8ac1939e39138101364400756af2bdee1da5\", GitTreeState:\"clean\", GoVersion:\"go1.23.5\"}",
    "stdout_lines": [
        "version.BuildInfo{Version:\"v3.17.1\", GitCommit:\"980d8ac1939e39138101364400756af2bdee1da5\", GitTreeState:\"clean\", GoVersion:\"go1.23.5\"}"
    ]
}

TASK [prereq : remove swap] ****************************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/prereq/tasks/main.yaml:2
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865079.6087668-24659-95571827935408 `" && echo ansible-tmp-1741865079.6087668-24659-95571827935408="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865079.6087668-24659-95571827935408 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmpktaln69c TO /home/ale/.ansible/tmp/ansible-tmp-1741865079.6087668-24659-95571827935408/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865079.6087668-24659-95571827935408/ /home/ale/.ansible/tmp/ansible-tmp-1741865079.6087668-24659-95571827935408/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-jgtblkprchruhfenwdlwbxlzwhovsfns ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865079.6087668-24659-95571827935408/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865079.6087668-24659-95571827935408/ > /dev/null 2>&1 && sleep 0'
changed: [localhost] => {
    "changed": true,
    "cmd": "swapoff -a",
    "delta": "0:00:00.005307",
    "end": "2025-03-13 12:24:39.826133",
    "invocation": {
        "module_args": {
            "_raw_params": "swapoff -a",
            "_uses_shell": true,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "",
    "rc": 0,
    "start": "2025-03-13 12:24:39.820826",
    "stderr": "",
    "stderr_lines": [],
    "stdout": "",
    "stdout_lines": []
}

TASK [prereq : Populate service facts] *****************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/prereq/tasks/main.yaml:5
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865079.9213529-24689-134927867648267 `" && echo ansible-tmp-1741865079.9213529-24689-134927867648267="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865079.9213529-24689-134927867648267 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/service_facts.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmpowqdpu9l TO /home/ale/.ansible/tmp/ansible-tmp-1741865079.9213529-24689-134927867648267/AnsiballZ_service_facts.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865079.9213529-24689-134927867648267/ /home/ale/.ansible/tmp/ansible-tmp-1741865079.9213529-24689-134927867648267/AnsiballZ_service_facts.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-cycketngfkkhmhkdeiyuvdfzdrxrdyud ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865079.9213529-24689-134927867648267/AnsiballZ_service_facts.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865079.9213529-24689-134927867648267/ > /dev/null 2>&1 && sleep 0'
ok: [localhost] => {
    "ansible_facts": {
        "services": {
            "ModemManager.service": {
                "name": "ModemManager.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "NetworkManager-dispatcher.service": {
                "name": "NetworkManager-dispatcher.service",
                "source": "systemd",
                "state": "inactive",
                "status": "enabled"
            },
            "NetworkManager-wait-online.service": {
                "name": "NetworkManager-wait-online.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "NetworkManager.service": {
                "name": "NetworkManager.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "accounts-daemon.service": {
                "name": "accounts-daemon.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "acpid": {
                "name": "acpid",
                "source": "sysv",
                "state": "running"
            },
            "acpid.service": {
                "name": "acpid.service",
                "source": "systemd",
                "state": "running",
                "status": "disabled"
            },
            "alsa-restore.service": {
                "name": "alsa-restore.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "alsa-state.service": {
                "name": "alsa-state.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "alsa-utils": {
                "name": "alsa-utils",
                "source": "sysv",
                "state": "stopped"
            },
            "alsa-utils.service": {
                "name": "alsa-utils.service",
                "source": "systemd",
                "state": "inactive",
                "status": "masked"
            },
            "anacron": {
                "name": "anacron",
                "source": "sysv",
                "state": "stopped"
            },
            "anacron.service": {
                "name": "anacron.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "apparmor": {
                "name": "apparmor",
                "source": "sysv",
                "state": "running"
            },
            "apparmor.service": {
                "name": "apparmor.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "apport": {
                "name": "apport",
                "source": "sysv",
                "state": "running"
            },
            "apport-autoreport.service": {
                "name": "apport-autoreport.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "apport-forward@.service": {
                "name": "apport-forward@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "static"
            },
            "apport.service": {
                "name": "apport.service",
                "source": "systemd",
                "state": "stopped",
                "status": "generated"
            },
            "apt-daily-upgrade.service": {
                "name": "apt-daily-upgrade.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "apt-daily.service": {
                "name": "apt-daily.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "apt-news.service": {
                "name": "apt-news.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "auditd.service": {
                "name": "auditd.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "auto-cpufreq.service": {
                "name": "auto-cpufreq.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "autovt@.service": {
                "name": "autovt@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "alias"
            },
            "avahi-daemon": {
                "name": "avahi-daemon",
                "source": "sysv",
                "state": "running"
            },
            "avahi-daemon.service": {
                "name": "avahi-daemon.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "bluetooth": {
                "name": "bluetooth",
                "source": "sysv",
                "state": "stopped"
            },
            "bluetooth.service": {
                "name": "bluetooth.service",
                "source": "systemd",
                "state": "inactive",
                "status": "enabled"
            },
            "bolt.service": {
                "name": "bolt.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "brltty-udev.service": {
                "name": "brltty-udev.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "brltty.service": {
                "name": "brltty.service",
                "source": "systemd",
                "state": "inactive",
                "status": "disabled"
            },
            "cloud-init-local.service": {
                "name": "cloud-init-local.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "colord.service": {
                "name": "colord.service",
                "source": "systemd",
                "state": "running",
                "status": "static"
            },
            "configure-printer@.service": {
                "name": "configure-printer@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "static"
            },
            "connman.service": {
                "name": "connman.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "console-getty.service": {
                "name": "console-getty.service",
                "source": "systemd",
                "state": "inactive",
                "status": "disabled"
            },
            "console-screen.service": {
                "name": "console-screen.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "console-setup.service": {
                "name": "console-setup.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "console-setup.sh": {
                "name": "console-setup.sh",
                "source": "sysv",
                "state": "stopped"
            },
            "container-getty@.service": {
                "name": "container-getty@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "static"
            },
            "containerd.service": {
                "name": "containerd.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "cron": {
                "name": "cron",
                "source": "sysv",
                "state": "running"
            },
            "cron.service": {
                "name": "cron.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "cryptdisks-early.service": {
                "name": "cryptdisks-early.service",
                "source": "systemd",
                "state": "inactive",
                "status": "masked"
            },
            "cryptdisks.service": {
                "name": "cryptdisks.service",
                "source": "systemd",
                "state": "inactive",
                "status": "masked"
            },
            "cups": {
                "name": "cups",
                "source": "sysv",
                "state": "running"
            },
            "cups-browsed": {
                "name": "cups-browsed",
                "source": "sysv",
                "state": "running"
            },
            "cups-browsed.service": {
                "name": "cups-browsed.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "cups.service": {
                "name": "cups.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "dbus": {
                "name": "dbus",
                "source": "sysv",
                "state": "running"
            },
            "dbus-fi.w1.wpa_supplicant1.service": {
                "name": "dbus-fi.w1.wpa_supplicant1.service",
                "source": "systemd",
                "state": "active",
                "status": "alias"
            },
            "dbus-org.bluez.service": {
                "name": "dbus-org.bluez.service",
                "source": "systemd",
                "state": "inactive",
                "status": "alias"
            },
            "dbus-org.freedesktop.Avahi.service": {
                "name": "dbus-org.freedesktop.Avahi.service",
                "source": "systemd",
                "state": "active",
                "status": "alias"
            },
            "dbus-org.freedesktop.ModemManager1.service": {
                "name": "dbus-org.freedesktop.ModemManager1.service",
                "source": "systemd",
                "state": "active",
                "status": "alias"
            },
            "dbus-org.freedesktop.hostname1.service": {
                "name": "dbus-org.freedesktop.hostname1.service",
                "source": "systemd",
                "state": "inactive",
                "status": "alias"
            },
            "dbus-org.freedesktop.locale1.service": {
                "name": "dbus-org.freedesktop.locale1.service",
                "source": "systemd",
                "state": "inactive",
                "status": "alias"
            },
            "dbus-org.freedesktop.login1.service": {
                "name": "dbus-org.freedesktop.login1.service",
                "source": "systemd",
                "state": "active",
                "status": "alias"
            },
            "dbus-org.freedesktop.nm-dispatcher.service": {
                "name": "dbus-org.freedesktop.nm-dispatcher.service",
                "source": "systemd",
                "state": "inactive",
                "status": "alias"
            },
            "dbus-org.freedesktop.oom1.service": {
                "name": "dbus-org.freedesktop.oom1.service",
                "source": "systemd",
                "state": "active",
                "status": "alias"
            },
            "dbus-org.freedesktop.resolve1.service": {
                "name": "dbus-org.freedesktop.resolve1.service",
                "source": "systemd",
                "state": "active",
                "status": "alias"
            },
            "dbus-org.freedesktop.thermald.service": {
                "name": "dbus-org.freedesktop.thermald.service",
                "source": "systemd",
                "state": "inactive",
                "status": "alias"
            },
            "dbus-org.freedesktop.timedate1.service": {
                "name": "dbus-org.freedesktop.timedate1.service",
                "source": "systemd",
                "state": "inactive",
                "status": "alias"
            },
            "dbus-org.freedesktop.timesync1.service": {
                "name": "dbus-org.freedesktop.timesync1.service",
                "source": "systemd",
                "state": "active",
                "status": "alias"
            },
            "dbus.service": {
                "name": "dbus.service",
                "source": "systemd",
                "state": "running",
                "status": "static"
            },
            "debug-shell.service": {
                "name": "debug-shell.service",
                "source": "systemd",
                "state": "inactive",
                "status": "disabled"
            },
            "display-manager.service": {
                "name": "display-manager.service",
                "source": "systemd",
                "state": "active",
                "status": "alias"
            },
            "dmesg.service": {
                "name": "dmesg.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "docker.service": {
                "name": "docker.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "dpkg-db-backup.service": {
                "name": "dpkg-db-backup.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "e2scrub@.service": {
                "name": "e2scrub@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "static"
            },
            "e2scrub_all.service": {
                "name": "e2scrub_all.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "e2scrub_fail@.service": {
                "name": "e2scrub_fail@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "static"
            },
            "e2scrub_reap.service": {
                "name": "e2scrub_reap.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "emergency.service": {
                "name": "emergency.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "esm-cache.service": {
                "name": "esm-cache.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "firewalld.service": {
                "name": "firewalld.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "fprintd.service": {
                "name": "fprintd.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "friendly-recovery.service": {
                "name": "friendly-recovery.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "fstrim.service": {
                "name": "fstrim.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "fwupd-offline-update.service": {
                "name": "fwupd-offline-update.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "fwupd-refresh.service": {
                "name": "fwupd-refresh.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "fwupd.service": {
                "name": "fwupd.service",
                "source": "systemd",
                "state": "running",
                "status": "static"
            },
            "gdm.service": {
                "name": "gdm.service",
                "source": "systemd",
                "state": "running",
                "status": "static"
            },
            "gdm3": {
                "name": "gdm3",
                "source": "sysv",
                "state": "running"
            },
            "gdm3.service": {
                "name": "gdm3.service",
                "source": "systemd",
                "state": "active",
                "status": "alias"
            },
            "geoclue.service": {
                "name": "geoclue.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "getty-static.service": {
                "name": "getty-static.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "getty@.service": {
                "name": "getty@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "enabled"
            },
            "getty@tty1.service": {
                "name": "getty@tty1.service",
                "source": "systemd",
                "state": "stopped",
                "status": "inactive"
            },
            "gpu-manager.service": {
                "name": "gpu-manager.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "grub-common": {
                "name": "grub-common",
                "source": "sysv",
                "state": "stopped"
            },
            "grub-common.service": {
                "name": "grub-common.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "grub-initrd-fallback.service": {
                "name": "grub-initrd-fallback.service",
                "source": "systemd",
                "state": "inactive",
                "status": "enabled"
            },
            "hwclock.service": {
                "name": "hwclock.service",
                "source": "systemd",
                "state": "inactive",
                "status": "masked"
            },
            "hwclock.sh": {
                "name": "hwclock.sh",
                "source": "sysv",
                "state": "stopped"
            },
            "iio-sensor-proxy.service": {
                "name": "iio-sensor-proxy.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "initrd-cleanup.service": {
                "name": "initrd-cleanup.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "initrd-parse-etc.service": {
                "name": "initrd-parse-etc.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "initrd-switch-root.service": {
                "name": "initrd-switch-root.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "initrd-udevadm-cleanup-db.service": {
                "name": "initrd-udevadm-cleanup-db.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "ipp-usb.service": {
                "name": "ipp-usb.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "irqbalance": {
                "name": "irqbalance",
                "source": "sysv",
                "state": "running"
            },
            "irqbalance.service": {
                "name": "irqbalance.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "k3s.service": {
                "name": "k3s.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "kbd.service": {
                "name": "kbd.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "kerneloops": {
                "name": "kerneloops",
                "source": "sysv",
                "state": "running"
            },
            "kerneloops.service": {
                "name": "kerneloops.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "keyboard-setup.service": {
                "name": "keyboard-setup.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "keyboard-setup.sh": {
                "name": "keyboard-setup.sh",
                "source": "sysv",
                "state": "stopped"
            },
            "kmod": {
                "name": "kmod",
                "source": "sysv",
                "state": "running"
            },
            "kmod-static-nodes.service": {
                "name": "kmod-static-nodes.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "kmod.service": {
                "name": "kmod.service",
                "source": "systemd",
                "state": "active",
                "status": "alias"
            },
            "loaded": {
                "name": "loaded",
                "source": "systemd",
                "state": "stopped",
                "status": "failed"
            },
            "logrotate.service": {
                "name": "logrotate.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "lxc.service": {
                "name": "lxc.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "man-db.service": {
                "name": "man-db.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "modprobe@.service": {
                "name": "modprobe@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "static"
            },
            "modprobe@configfs.service": {
                "name": "modprobe@configfs.service",
                "source": "systemd",
                "state": "stopped",
                "status": "inactive"
            },
            "modprobe@drm.service": {
                "name": "modprobe@drm.service",
                "source": "systemd",
                "state": "stopped",
                "status": "inactive"
            },
            "modprobe@efi_pstore.service": {
                "name": "modprobe@efi_pstore.service",
                "source": "systemd",
                "state": "stopped",
                "status": "inactive"
            },
            "modprobe@fuse.service": {
                "name": "modprobe@fuse.service",
                "source": "systemd",
                "state": "stopped",
                "status": "inactive"
            },
            "motd-news.service": {
                "name": "motd-news.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "netplan-ovs-cleanup.service": {
                "name": "netplan-ovs-cleanup.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled-runtime"
            },
            "networkd-dispatcher.service": {
                "name": "networkd-dispatcher.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "nftables.service": {
                "name": "nftables.service",
                "source": "systemd",
                "state": "inactive",
                "status": "disabled"
            },
            "nm-priv-helper.service": {
                "name": "nm-priv-helper.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "nslcd.service": {
                "name": "nslcd.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "oem-config.service": {
                "name": "oem-config.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "open-vm-tools": {
                "name": "open-vm-tools",
                "source": "sysv",
                "state": "stopped"
            },
            "open-vm-tools.service": {
                "name": "open-vm-tools.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "openvpn": {
                "name": "openvpn",
                "source": "sysv",
                "state": "running"
            },
            "openvpn-client@.service": {
                "name": "openvpn-client@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "disabled"
            },
            "openvpn-server@.service": {
                "name": "openvpn-server@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "disabled"
            },
            "openvpn.service": {
                "name": "openvpn.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "openvpn@.service": {
                "name": "openvpn@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "disabled"
            },
            "ovsdb-server.service": {
                "name": "ovsdb-server.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "packagekit-offline-update.service": {
                "name": "packagekit-offline-update.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "packagekit.service": {
                "name": "packagekit.service",
                "source": "systemd",
                "state": "running",
                "status": "static"
            },
            "plymouth": {
                "name": "plymouth",
                "source": "sysv",
                "state": "stopped"
            },
            "plymouth-halt.service": {
                "name": "plymouth-halt.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "plymouth-kexec.service": {
                "name": "plymouth-kexec.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "plymouth-log": {
                "name": "plymouth-log",
                "source": "sysv",
                "state": "running"
            },
            "plymouth-log.service": {
                "name": "plymouth-log.service",
                "source": "systemd",
                "state": "active",
                "status": "alias"
            },
            "plymouth-poweroff.service": {
                "name": "plymouth-poweroff.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "plymouth-quit-wait.service": {
                "name": "plymouth-quit-wait.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "plymouth-quit.service": {
                "name": "plymouth-quit.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "plymouth-read-write.service": {
                "name": "plymouth-read-write.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "plymouth-reboot.service": {
                "name": "plymouth-reboot.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "plymouth-start.service": {
                "name": "plymouth-start.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "plymouth-switch-root-initramfs.service": {
                "name": "plymouth-switch-root-initramfs.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "plymouth-switch-root.service": {
                "name": "plymouth-switch-root.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "plymouth.service": {
                "name": "plymouth.service",
                "source": "systemd",
                "state": "inactive",
                "status": "alias"
            },
            "polkit.service": {
                "name": "polkit.service",
                "source": "systemd",
                "state": "running",
                "status": "static"
            },
            "power-profiles-daemon.service": {
                "name": "power-profiles-daemon.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "procps": {
                "name": "procps",
                "source": "sysv",
                "state": "running"
            },
            "procps.service": {
                "name": "procps.service",
                "source": "systemd",
                "state": "active",
                "status": "alias"
            },
            "pulseaudio-enable-autospawn": {
                "name": "pulseaudio-enable-autospawn",
                "source": "sysv",
                "state": "stopped"
            },
            "pulseaudio-enable-autospawn.service": {
                "name": "pulseaudio-enable-autospawn.service",
                "source": "systemd",
                "state": "inactive",
                "status": "masked"
            },
            "quotaon.service": {
                "name": "quotaon.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "rc-local.service": {
                "name": "rc-local.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "rc.service": {
                "name": "rc.service",
                "source": "systemd",
                "state": "inactive",
                "status": "masked"
            },
            "rcS.service": {
                "name": "rcS.service",
                "source": "systemd",
                "state": "inactive",
                "status": "masked"
            },
            "rescue.service": {
                "name": "rescue.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "rsync": {
                "name": "rsync",
                "source": "sysv",
                "state": "stopped"
            },
            "rsync.service": {
                "name": "rsync.service",
                "source": "systemd",
                "state": "inactive",
                "status": "disabled"
            },
            "rsyslog.service": {
                "name": "rsyslog.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "rtkit-daemon.service": {
                "name": "rtkit-daemon.service",
                "source": "systemd",
                "state": "running",
                "status": "disabled"
            },
            "saned": {
                "name": "saned",
                "source": "sysv",
                "state": "stopped"
            },
            "saned.service": {
                "name": "saned.service",
                "source": "systemd",
                "state": "inactive",
                "status": "masked"
            },
            "saned@.service": {
                "name": "saned@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "indirect"
            },
            "secureboot-db.service": {
                "name": "secureboot-db.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "serial-getty@.service": {
                "name": "serial-getty@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "disabled"
            },
            "setvtrgb.service": {
                "name": "setvtrgb.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "snapd.apparmor.service": {
                "name": "snapd.apparmor.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "snapd.autoimport.service": {
                "name": "snapd.autoimport.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "snapd.core-fixup.service": {
                "name": "snapd.core-fixup.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "snapd.failure.service": {
                "name": "snapd.failure.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "snapd.recovery-chooser-trigger.service": {
                "name": "snapd.recovery-chooser-trigger.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "snapd.seeded.service": {
                "name": "snapd.seeded.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "snapd.service": {
                "name": "snapd.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "snapd.snap-repair.service": {
                "name": "snapd.snap-repair.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "snapd.system-shutdown.service": {
                "name": "snapd.system-shutdown.service",
                "source": "systemd",
                "state": "inactive",
                "status": "enabled"
            },
            "speech-dispatcher": {
                "name": "speech-dispatcher",
                "source": "sysv",
                "state": "stopped"
            },
            "speech-dispatcher.service": {
                "name": "speech-dispatcher.service",
                "source": "systemd",
                "state": "inactive",
                "status": "generated"
            },
            "speech-dispatcherd.service": {
                "name": "speech-dispatcherd.service",
                "source": "systemd",
                "state": "inactive",
                "status": "disabled"
            },
            "spice-vdagent": {
                "name": "spice-vdagent",
                "source": "sysv",
                "state": "stopped"
            },
            "spice-vdagent.service": {
                "name": "spice-vdagent.service",
                "source": "systemd",
                "state": "inactive",
                "status": "alias"
            },
            "spice-vdagentd.service": {
                "name": "spice-vdagentd.service",
                "source": "systemd",
                "state": "inactive",
                "status": "indirect"
            },
            "sudo.service": {
                "name": "sudo.service",
                "source": "systemd",
                "state": "inactive",
                "status": "masked"
            },
            "switcheroo-control.service": {
                "name": "switcheroo-control.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "syslog.service": {
                "name": "syslog.service",
                "source": "systemd",
                "state": "active",
                "status": "alias"
            },
            "system-update-cleanup.service": {
                "name": "system-update-cleanup.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "system76-power.service": {
                "name": "system76-power.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "systemd-ask-password-console.service": {
                "name": "systemd-ask-password-console.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-ask-password-plymouth.service": {
                "name": "systemd-ask-password-plymouth.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-ask-password-wall.service": {
                "name": "systemd-ask-password-wall.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-backlight@.service": {
                "name": "systemd-backlight@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "static"
            },
            "systemd-binfmt.service": {
                "name": "systemd-binfmt.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-bless-boot.service": {
                "name": "systemd-bless-boot.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "systemd-boot-check-no-failures.service": {
                "name": "systemd-boot-check-no-failures.service",
                "source": "systemd",
                "state": "inactive",
                "status": "disabled"
            },
            "systemd-boot-system-token.service": {
                "name": "systemd-boot-system-token.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-exit.service": {
                "name": "systemd-exit.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "systemd-fsck-root.service": {
                "name": "systemd-fsck-root.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled-runtime"
            },
            "systemd-fsck@.service": {
                "name": "systemd-fsck@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "static"
            },
            "systemd-fsck@dev-disk-by\\x2duuid-ED07\\x2dDB04.service": {
                "name": "systemd-fsck@dev-disk-by\\x2duuid-ED07\\x2dDB04.service",
                "source": "systemd",
                "state": "stopped",
                "status": "active"
            },
            "systemd-fsckd.service": {
                "name": "systemd-fsckd.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-halt.service": {
                "name": "systemd-halt.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "systemd-hibernate-resume@.service": {
                "name": "systemd-hibernate-resume@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "static"
            },
            "systemd-hibernate.service": {
                "name": "systemd-hibernate.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "systemd-hostnamed.service": {
                "name": "systemd-hostnamed.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "systemd-hwdb-update.service": {
                "name": "systemd-hwdb-update.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "systemd-hybrid-sleep.service": {
                "name": "systemd-hybrid-sleep.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "systemd-initctl.service": {
                "name": "systemd-initctl.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-journal-flush.service": {
                "name": "systemd-journal-flush.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-journald.service": {
                "name": "systemd-journald.service",
                "source": "systemd",
                "state": "running",
                "status": "static"
            },
            "systemd-journald@.service": {
                "name": "systemd-journald@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "static"
            },
            "systemd-kexec.service": {
                "name": "systemd-kexec.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "systemd-localed.service": {
                "name": "systemd-localed.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "systemd-logind.service": {
                "name": "systemd-logind.service",
                "source": "systemd",
                "state": "running",
                "status": "static"
            },
            "systemd-machine-id-commit.service": {
                "name": "systemd-machine-id-commit.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-modules-load.service": {
                "name": "systemd-modules-load.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-network-generator.service": {
                "name": "systemd-network-generator.service",
                "source": "systemd",
                "state": "inactive",
                "status": "disabled"
            },
            "systemd-networkd-wait-online.service": {
                "name": "systemd-networkd-wait-online.service",
                "source": "systemd",
                "state": "inactive",
                "status": "disabled"
            },
            "systemd-networkd.service": {
                "name": "systemd-networkd.service",
                "source": "systemd",
                "state": "stopped",
                "status": "disabled"
            },
            "systemd-oomd.service": {
                "name": "systemd-oomd.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "systemd-poweroff.service": {
                "name": "systemd-poweroff.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "systemd-pstore.service": {
                "name": "systemd-pstore.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "systemd-quotacheck.service": {
                "name": "systemd-quotacheck.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-random-seed.service": {
                "name": "systemd-random-seed.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-reboot.service": {
                "name": "systemd-reboot.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "systemd-remount-fs.service": {
                "name": "systemd-remount-fs.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled-runtime"
            },
            "systemd-resolved.service": {
                "name": "systemd-resolved.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "systemd-rfkill.service": {
                "name": "systemd-rfkill.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-suspend-then-hibernate.service": {
                "name": "systemd-suspend-then-hibernate.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "systemd-suspend.service": {
                "name": "systemd-suspend.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "systemd-sysctl.service": {
                "name": "systemd-sysctl.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-sysext.service": {
                "name": "systemd-sysext.service",
                "source": "systemd",
                "state": "inactive",
                "status": "disabled"
            },
            "systemd-sysusers.service": {
                "name": "systemd-sysusers.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-time-wait-sync.service": {
                "name": "systemd-time-wait-sync.service",
                "source": "systemd",
                "state": "inactive",
                "status": "disabled"
            },
            "systemd-timedated.service": {
                "name": "systemd-timedated.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "systemd-timesyncd.service": {
                "name": "systemd-timesyncd.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "systemd-tmpfiles-clean.service": {
                "name": "systemd-tmpfiles-clean.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-tmpfiles-setup-dev.service": {
                "name": "systemd-tmpfiles-setup-dev.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-tmpfiles-setup.service": {
                "name": "systemd-tmpfiles-setup.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-udev-settle.service": {
                "name": "systemd-udev-settle.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "systemd-udev-trigger.service": {
                "name": "systemd-udev-trigger.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-udevd.service": {
                "name": "systemd-udevd.service",
                "source": "systemd",
                "state": "running",
                "status": "static"
            },
            "systemd-update-done.service": {
                "name": "systemd-update-done.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "systemd-update-utmp-runlevel.service": {
                "name": "systemd-update-utmp-runlevel.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-update-utmp.service": {
                "name": "systemd-update-utmp.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-user-sessions.service": {
                "name": "systemd-user-sessions.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "systemd-vconsole-setup.service": {
                "name": "systemd-vconsole-setup.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "systemd-volatile-root.service": {
                "name": "systemd-volatile-root.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "thermald.service": {
                "name": "thermald.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "tuned.service": {
                "name": "tuned.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "ua-auto-attach.service": {
                "name": "ua-auto-attach.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "ua-reboot-cmds.service": {
                "name": "ua-reboot-cmds.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "ua-timer.service": {
                "name": "ua-timer.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "ubuntu-advantage-cloud-id-shim.service": {
                "name": "ubuntu-advantage-cloud-id-shim.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            },
            "ubuntu-advantage-desktop-daemon.service": {
                "name": "ubuntu-advantage-desktop-daemon.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "ubuntu-advantage.service": {
                "name": "ubuntu-advantage.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "ubuntu-fan": {
                "name": "ubuntu-fan",
                "source": "sysv",
                "state": "running"
            },
            "ubuntu-fan.service": {
                "name": "ubuntu-fan.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "udev": {
                "name": "udev",
                "source": "sysv",
                "state": "running"
            },
            "udev.service": {
                "name": "udev.service",
                "source": "systemd",
                "state": "active",
                "status": "alias"
            },
            "udisks2.service": {
                "name": "udisks2.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "ufw": {
                "name": "ufw",
                "source": "sysv",
                "state": "running"
            },
            "ufw.service": {
                "name": "ufw.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "unattended-upgrades": {
                "name": "unattended-upgrades",
                "source": "sysv",
                "state": "running"
            },
            "unattended-upgrades.service": {
                "name": "unattended-upgrades.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "update-notifier-download.service": {
                "name": "update-notifier-download.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "update-notifier-motd.service": {
                "name": "update-notifier-motd.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "upower.service": {
                "name": "upower.service",
                "source": "systemd",
                "state": "running",
                "status": "disabled"
            },
            "usb_modeswitch@.service": {
                "name": "usb_modeswitch@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "static"
            },
            "usbmuxd.service": {
                "name": "usbmuxd.service",
                "source": "systemd",
                "state": "inactive",
                "status": "static"
            },
            "user-runtime-dir@.service": {
                "name": "user-runtime-dir@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "static"
            },
            "user-runtime-dir@1000.service": {
                "name": "user-runtime-dir@1000.service",
                "source": "systemd",
                "state": "stopped",
                "status": "active"
            },
            "user@.service": {
                "name": "user@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "static"
            },
            "user@1000.service": {
                "name": "user@1000.service",
                "source": "systemd",
                "state": "running",
                "status": "active"
            },
            "uuidd": {
                "name": "uuidd",
                "source": "sysv",
                "state": "running"
            },
            "uuidd.service": {
                "name": "uuidd.service",
                "source": "systemd",
                "state": "running",
                "status": "indirect"
            },
            "vboxadd-service.service": {
                "name": "vboxadd-service.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "vboxadd.service": {
                "name": "vboxadd.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "vgauth.service": {
                "name": "vgauth.service",
                "source": "systemd",
                "state": "stopped",
                "status": "enabled"
            },
            "vmtoolsd.service": {
                "name": "vmtoolsd.service",
                "source": "systemd",
                "state": "inactive",
                "status": "alias"
            },
            "wacom-inputattach@.service": {
                "name": "wacom-inputattach@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "static"
            },
            "whoopsie": {
                "name": "whoopsie",
                "source": "sysv",
                "state": "stopped"
            },
            "whoopsie.service": {
                "name": "whoopsie.service",
                "source": "systemd",
                "state": "stopped",
                "status": "static"
            },
            "wpa_supplicant-nl80211@.service": {
                "name": "wpa_supplicant-nl80211@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "disabled"
            },
            "wpa_supplicant-wired@.service": {
                "name": "wpa_supplicant-wired@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "disabled"
            },
            "wpa_supplicant.service": {
                "name": "wpa_supplicant.service",
                "source": "systemd",
                "state": "running",
                "status": "enabled"
            },
            "wpa_supplicant@.service": {
                "name": "wpa_supplicant@.service",
                "source": "systemd",
                "state": "unknown",
                "status": "disabled"
            },
            "x11-common": {
                "name": "x11-common",
                "source": "sysv",
                "state": "stopped"
            },
            "x11-common.service": {
                "name": "x11-common.service",
                "source": "systemd",
                "state": "inactive",
                "status": "masked"
            },
            "zfs-mount.service": {
                "name": "zfs-mount.service",
                "source": "systemd",
                "state": "stopped",
                "status": "not-found"
            }
        }
    },
    "changed": false,
    "invocation": {
        "module_args": {}
    }
}

TASK [prereq : Get ufw status] *************************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/prereq/tasks/main.yaml:14
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865084.2465923-25251-74798467545778 `" && echo ansible-tmp-1741865084.2465923-25251-74798467545778="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865084.2465923-25251-74798467545778 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmpx9whbtex TO /home/ale/.ansible/tmp/ansible-tmp-1741865084.2465923-25251-74798467545778/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865084.2465923-25251-74798467545778/ /home/ale/.ansible/tmp/ansible-tmp-1741865084.2465923-25251-74798467545778/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-taodfwusuzbscgpcqvaxvpjwstoroipn ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865084.2465923-25251-74798467545778/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865084.2465923-25251-74798467545778/ > /dev/null 2>&1 && sleep 0'
ok: [localhost] => {
    "changed": false,
    "cmd": [
        "ufw",
        "status"
    ],
    "delta": "0:00:00.086167",
    "end": "2025-03-13 12:24:44.548910",
    "invocation": {
        "module_args": {
            "_raw_params": "ufw status",
            "_uses_shell": false,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "",
    "rc": 0,
    "start": "2025-03-13 12:24:44.462743",
    "stderr": "",
    "stderr_lines": [],
    "stdout": "Status: inactive",
    "stdout_lines": [
        "Status: inactive"
    ]
}

TASK [prereq : If ufw enabled, open api port] **********************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/prereq/tasks/main.yaml:20
skipping: [localhost] => {
    "changed": false,
    "false_condition": "'Status: active' in ufw_status.stdout",
    "skip_reason": "Conditional result was False"
}

TASK [prereq : If ufw enabled, allow default CIDRs] ****************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/prereq/tasks/main.yaml:27
skipping: [localhost] => (item=10.42.0.0/16)  => {
    "ansible_loop_var": "item",
    "changed": false,
    "false_condition": "'Status: active' in ufw_status.stdout",
    "item": "10.42.0.0/16",
    "skip_reason": "Conditional result was False"
}
skipping: [localhost] => (item=10.43.0.0/16)  => {
    "ansible_loop_var": "item",
    "changed": false,
    "false_condition": "'Status: active' in ufw_status.stdout",
    "item": "10.43.0.0/16",
    "skip_reason": "Conditional result was False"
}
skipping: [localhost] => {
    "changed": false,
    "msg": "All items skipped"
}

TASK [prereq : If firewalld enabled, open api port] ****************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/prereq/tasks/main.yaml:39
skipping: [localhost] => {
    "changed": false,
    "false_condition": "ansible_facts.services['firewalld.service'].state == 'running'",
    "skip_reason": "Conditional result was False"
}

TASK [prereq : If firewalld enabled, allow default CIDRs] **********************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/prereq/tasks/main.yaml:47
skipping: [localhost] => (item=10.42.0.0/16)  => {
    "ansible_loop_var": "item",
    "changed": false,
    "false_condition": "ansible_facts.services['firewalld.service'].state == 'running'",
    "item": "10.42.0.0/16",
    "skip_reason": "Conditional result was False"
}
skipping: [localhost] => (item=10.43.0.0/16)  => {
    "ansible_loop_var": "item",
    "changed": false,
    "false_condition": "ansible_facts.services['firewalld.service'].state == 'running'",
    "item": "10.43.0.0/16",
    "skip_reason": "Conditional result was False"
}
skipping: [localhost] => {
    "changed": false,
    "msg": "All items skipped"
}

TASK [helm_installation : Install helm] ****************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/helm_installation/tasks/main.yaml:1
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865085.0272226-25286-90230495719735 `" && echo ansible-tmp-1741865085.0272226-25286-90230495719735="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865085.0272226-25286-90230495719735 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmpiqrf9u29 TO /home/ale/.ansible/tmp/ansible-tmp-1741865085.0272226-25286-90230495719735/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865085.0272226-25286-90230495719735/ /home/ale/.ansible/tmp/ansible-tmp-1741865085.0272226-25286-90230495719735/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-gnapqslzoqaesplslmnussviqvqeheks ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865085.0272226-25286-90230495719735/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865085.0272226-25286-90230495719735/ > /dev/null 2>&1 && sleep 0'
changed: [localhost] => {
    "changed": true,
    "cmd": "curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm",
    "delta": "0:00:05.034529",
    "end": "2025-03-13 12:24:50.310253",
    "invocation": {
        "module_args": {
            "_raw_params": "curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm",
            "_uses_shell": true,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "",
    "rc": 0,
    "start": "2025-03-13 12:24:45.275724",
    "stderr": "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1699  100  1699    0     0   6354      0 --:--:-- --:--:-- --:--:--  6363",
    "stderr_lines": [
        "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current",
        "                                 Dload  Upload   Total   Spent    Left  Speed",
        "",
        "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0",
        "100  1699  100  1699    0     0   6354      0 --:--:-- --:--:-- --:--:--  6363"
    ],
    "stdout": "Reading package lists...\nBuilding dependency tree...\nReading state information...\napt-transport-https is already the newest version (2.4.13).\n0 upgraded, 0 newly installed, 0 to remove and 176 not upgraded.\ndeb [arch=amd64 signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\nHit:1 https://packages.microsoft.com/repos/code stable InRelease\nHit:2 http://it.archive.ubuntu.com/ubuntu jammy InRelease\nHit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\nHit:4 https://baltocdn.com/helm/stable/debian all InRelease\nHit:5 http://it.archive.ubuntu.com/ubuntu jammy-updates InRelease\nHit:6 http://it.archive.ubuntu.com/ubuntu jammy-backports InRelease\nHit:7 https://ppa.launchpadcontent.net/ansible/ansible/ubuntu jammy InRelease\nReading package lists...\nReading package lists...\nBuilding dependency tree...\nReading state information...\nhelm is already the newest version (3.17.1-1).\n0 upgraded, 0 newly installed, 0 to remove and 176 not upgraded.",
    "stdout_lines": [
        "Reading package lists...",
        "Building dependency tree...",
        "Reading state information...",
        "apt-transport-https is already the newest version (2.4.13).",
        "0 upgraded, 0 newly installed, 0 to remove and 176 not upgraded.",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main",
        "Hit:1 https://packages.microsoft.com/repos/code stable InRelease",
        "Hit:2 http://it.archive.ubuntu.com/ubuntu jammy InRelease",
        "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease",
        "Hit:4 https://baltocdn.com/helm/stable/debian all InRelease",
        "Hit:5 http://it.archive.ubuntu.com/ubuntu jammy-updates InRelease",
        "Hit:6 http://it.archive.ubuntu.com/ubuntu jammy-backports InRelease",
        "Hit:7 https://ppa.launchpadcontent.net/ansible/ansible/ubuntu jammy InRelease",
        "Reading package lists...",
        "Reading package lists...",
        "Building dependency tree...",
        "Reading state information...",
        "helm is already the newest version (3.17.1-1).",
        "0 upgraded, 0 newly installed, 0 to remove and 176 not upgraded."
    ]
}

TASK [k3s : Install k3s without traefik] ***************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/k3s/tasks/main.yaml:3
skipping: [localhost] => {
    "changed": false,
    "false_condition": "k3s_version_output.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [k3s : Change ownership of k3s] *******************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/k3s/tasks/main.yaml:6
skipping: [localhost] => {
    "changed": false,
    "false_condition": "k3s_version_output.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [k3s : Change access to k3s] **********************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/k3s/tasks/main.yaml:9
skipping: [localhost] => {
    "changed": false,
    "false_condition": "k3s_version_output.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [nginx_installation : Installation of nginx Ingress Controller] ***********
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/nginx_installation/tasks/nginx_ingressController.yaml:3
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865090.5691462-25853-191430328797048 `" && echo ansible-tmp-1741865090.5691462-25853-191430328797048="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865090.5691462-25853-191430328797048 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmp9sr8sy7r TO /home/ale/.ansible/tmp/ansible-tmp-1741865090.5691462-25853-191430328797048/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865090.5691462-25853-191430328797048/ /home/ale/.ansible/tmp/ansible-tmp-1741865090.5691462-25853-191430328797048/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-hxbcjotcahmgiezgbwvdcvrpbvbrxpof ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865090.5691462-25853-191430328797048/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865090.5691462-25853-191430328797048/ > /dev/null 2>&1 && sleep 0'
changed: [localhost] => {
    "changed": true,
    "cmd": [
        "kubectl",
        "apply",
        "-f",
        "https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.1/deploy/static/provider/baremetal/deploy.yaml"
    ],
    "delta": "0:00:02.112952",
    "end": "2025-03-13 12:24:52.892567",
    "invocation": {
        "module_args": {
            "_raw_params": "kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.1/deploy/static/provider/baremetal/deploy.yaml",
            "_uses_shell": false,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "",
    "rc": 0,
    "start": "2025-03-13 12:24:50.779615",
    "stderr": "",
    "stderr_lines": [],
    "stdout": "namespace/ingress-nginx unchanged\nserviceaccount/ingress-nginx unchanged\nconfigmap/ingress-nginx-controller unchanged\nclusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged\nclusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged\nrole.rbac.authorization.k8s.io/ingress-nginx unchanged\nrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged\nservice/ingress-nginx-controller-admission unchanged\nservice/ingress-nginx-controller configured\ndeployment.apps/ingress-nginx-controller configured\ningressclass.networking.k8s.io/nginx unchanged\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured\nserviceaccount/ingress-nginx-admission unchanged\nclusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged\nclusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged\nrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged\nrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged\njob.batch/ingress-nginx-admission-create unchanged\njob.batch/ingress-nginx-admission-patch unchanged",
    "stdout_lines": [
        "namespace/ingress-nginx unchanged",
        "serviceaccount/ingress-nginx unchanged",
        "configmap/ingress-nginx-controller unchanged",
        "clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged",
        "clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged",
        "role.rbac.authorization.k8s.io/ingress-nginx unchanged",
        "rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged",
        "service/ingress-nginx-controller-admission unchanged",
        "service/ingress-nginx-controller configured",
        "deployment.apps/ingress-nginx-controller configured",
        "ingressclass.networking.k8s.io/nginx unchanged",
        "validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured",
        "serviceaccount/ingress-nginx-admission unchanged",
        "clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged",
        "clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged",
        "role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged",
        "rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged",
        "job.batch/ingress-nginx-admission-create unchanged",
        "job.batch/ingress-nginx-admission-patch unchanged"
    ]
}

TASK [nginx_installation : Configure the nginx Ingress Controller as the default one] ***
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/nginx_installation/tasks/nginx_ingressController.yaml:6
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865092.972964-26063-153494023806873 `" && echo ansible-tmp-1741865092.972964-26063-153494023806873="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865092.972964-26063-153494023806873 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmp7ohw7hvu TO /home/ale/.ansible/tmp/ansible-tmp-1741865092.972964-26063-153494023806873/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865092.972964-26063-153494023806873/ /home/ale/.ansible/tmp/ansible-tmp-1741865092.972964-26063-153494023806873/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-qtwbnuhbbwdrxboxqtkkwppvhafydazn ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865092.972964-26063-153494023806873/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865092.972964-26063-153494023806873/ > /dev/null 2>&1 && sleep 0'
changed: [localhost] => {
    "changed": true,
    "cmd": [
        "kubectl",
        "annotate",
        "ingressclass",
        "nginx",
        "ingressclass.kubernetes.io/is-default-class=true"
    ],
    "delta": "0:00:00.192005",
    "end": "2025-03-13 12:24:53.384993",
    "invocation": {
        "module_args": {
            "_raw_params": "kubectl annotate ingressclass nginx ingressclass.kubernetes.io/is-default-class=true",
            "_uses_shell": false,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "",
    "rc": 0,
    "start": "2025-03-13 12:24:53.192988",
    "stderr": "",
    "stderr_lines": [],
    "stdout": "ingressclass.networking.k8s.io/nginx annotated",
    "stdout_lines": [
        "ingressclass.networking.k8s.io/nginx annotated"
    ]
}

TASK [nginx_installation : Copy LoadBalancer Configuration File in home directory] ***
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/nginx_installation/tasks/nginx_ingressController.yaml:9
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865093.4589074-26104-177333957877146 `" && echo ansible-tmp-1741865093.4589074-26104-177333957877146="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865093.4589074-26104-177333957877146 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/stat.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmpoailh150 TO /home/ale/.ansible/tmp/ansible-tmp-1741865093.4589074-26104-177333957877146/AnsiballZ_stat.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865093.4589074-26104-177333957877146/ /home/ale/.ansible/tmp/ansible-tmp-1741865093.4589074-26104-177333957877146/AnsiballZ_stat.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-ibroupccaqipbxkmnmjfvaxqhrjjtakt ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865093.4589074-26104-177333957877146/AnsiballZ_stat.py'"'"' && sleep 0'
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmp920ly98d/loadbalancer.j2 TO /home/ale/.ansible/tmp/ansible-tmp-1741865093.4589074-26104-177333957877146/.source.yaml
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865093.4589074-26104-177333957877146/ /home/ale/.ansible/tmp/ansible-tmp-1741865093.4589074-26104-177333957877146/.source.yaml && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/copy.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmp6no4xy6k TO /home/ale/.ansible/tmp/ansible-tmp-1741865093.4589074-26104-177333957877146/AnsiballZ_copy.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865093.4589074-26104-177333957877146/ /home/ale/.ansible/tmp/ansible-tmp-1741865093.4589074-26104-177333957877146/AnsiballZ_copy.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-ezbiglguflfurcqybnxqpvziruqxffzm ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865093.4589074-26104-177333957877146/AnsiballZ_copy.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865093.4589074-26104-177333957877146/ > /dev/null 2>&1 && sleep 0'
changed: [localhost] => {
    "changed": true,
    "checksum": "e784f298e464ce8cc7207a3a96b713bf828bcb00",
    "dest": "./loadbalancer.yaml",
    "diff": [],
    "gid": 0,
    "group": "root",
    "invocation": {
        "module_args": {
            "_original_basename": "loadbalancer.j2",
            "attributes": null,
            "backup": false,
            "checksum": "e784f298e464ce8cc7207a3a96b713bf828bcb00",
            "content": null,
            "dest": "./loadbalancer.yaml",
            "directory_mode": null,
            "follow": false,
            "force": true,
            "group": null,
            "local_follow": null,
            "mode": null,
            "owner": null,
            "remote_src": null,
            "selevel": null,
            "serole": null,
            "setype": null,
            "seuser": null,
            "src": "/home/ale/.ansible/tmp/ansible-tmp-1741865093.4589074-26104-177333957877146/.source.yaml",
            "unsafe_writes": false,
            "validate": null
        }
    },
    "md5sum": "c71a3334136fc8ad03b363f9bb778198",
    "mode": "0644",
    "owner": "root",
    "size": 401,
    "src": "/home/ale/.ansible/tmp/ansible-tmp-1741865093.4589074-26104-177333957877146/.source.yaml",
    "state": "file",
    "uid": 0
}

TASK [nginx_installation : Apply LoadBalancer Configuration File] **************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/nginx_installation/tasks/nginx_ingressController.yaml:14
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865094.309294-26174-29854132453932 `" && echo ansible-tmp-1741865094.309294-26174-29854132453932="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865094.309294-26174-29854132453932 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmplxnlhhjv TO /home/ale/.ansible/tmp/ansible-tmp-1741865094.309294-26174-29854132453932/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865094.309294-26174-29854132453932/ /home/ale/.ansible/tmp/ansible-tmp-1741865094.309294-26174-29854132453932/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-iqwbzpnwogiwnqiehwvqegtvaapecmrw ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865094.309294-26174-29854132453932/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865094.309294-26174-29854132453932/ > /dev/null 2>&1 && sleep 0'
changed: [localhost] => {
    "changed": true,
    "cmd": [
        "kubectl",
        "apply",
        "-f",
        "loadbalancer.yaml"
    ],
    "delta": "0:00:00.427391",
    "end": "2025-03-13 12:24:54.982747",
    "invocation": {
        "module_args": {
            "_raw_params": "kubectl apply -f loadbalancer.yaml",
            "_uses_shell": false,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "",
    "rc": 0,
    "start": "2025-03-13 12:24:54.555356",
    "stderr": "",
    "stderr_lines": [],
    "stdout": "service/ingress-nginx-controller configured",
    "stdout_lines": [
        "service/ingress-nginx-controller configured"
    ]
}

TASK [nginx_installation : Clean LoadBalancer Configuration File] **************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/nginx_installation/tasks/nginx_ingressController.yaml:17
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865095.161612-26219-114749195742993 `" && echo ansible-tmp-1741865095.161612-26219-114749195742993="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865095.161612-26219-114749195742993 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/file.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmplukpisq9 TO /home/ale/.ansible/tmp/ansible-tmp-1741865095.161612-26219-114749195742993/AnsiballZ_file.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865095.161612-26219-114749195742993/ /home/ale/.ansible/tmp/ansible-tmp-1741865095.161612-26219-114749195742993/AnsiballZ_file.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-rfxjxighsygdfgokasvidnlwcjlevzox ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865095.161612-26219-114749195742993/AnsiballZ_file.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865095.161612-26219-114749195742993/ > /dev/null 2>&1 && sleep 0'
changed: [localhost] => {
    "changed": true,
    "diff": {
        "after": {
            "path": "loadbalancer.yaml",
            "state": "absent"
        },
        "before": {
            "path": "loadbalancer.yaml",
            "state": "file"
        }
    },
    "invocation": {
        "module_args": {
            "_diff_peek": null,
            "_original_basename": null,
            "access_time": null,
            "access_time_format": "%Y%m%d%H%M.%S",
            "attributes": null,
            "follow": true,
            "force": false,
            "group": null,
            "mode": null,
            "modification_time": null,
            "modification_time_format": "%Y%m%d%H%M.%S",
            "owner": null,
            "path": "loadbalancer.yaml",
            "recurse": false,
            "selevel": null,
            "serole": null,
            "setype": null,
            "seuser": null,
            "src": null,
            "state": "absent",
            "unsafe_writes": false
        }
    },
    "path": "loadbalancer.yaml",
    "state": "absent"
}

TASK [energymon : Check if prometheus and grafana are already installed] *******
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/prometheus-grafana.yaml:2
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865096.1794155-26469-137151143055804 `" && echo ansible-tmp-1741865096.1794155-26469-137151143055804="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865096.1794155-26469-137151143055804 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmp4etwjqmc TO /home/ale/.ansible/tmp/ansible-tmp-1741865096.1794155-26469-137151143055804/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865096.1794155-26469-137151143055804/ /home/ale/.ansible/tmp/ansible-tmp-1741865096.1794155-26469-137151143055804/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-eaucnsvelzlkurfpeugozdlwlkmtttix ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865096.1794155-26469-137151143055804/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865096.1794155-26469-137151143055804/ > /dev/null 2>&1 && sleep 0'
changed: [localhost] => {
    "changed": true,
    "cmd": "helm --kubeconfig /etc/rancher/k3s/k3s.yaml status prometheus -n monitoring",
    "delta": "0:00:00.158519",
    "end": "2025-03-13 12:24:56.563259",
    "invocation": {
        "module_args": {
            "_raw_params": "helm --kubeconfig /etc/rancher/k3s/k3s.yaml status prometheus -n monitoring",
            "_uses_shell": true,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "",
    "rc": 0,
    "start": "2025-03-13 12:24:56.404740",
    "stderr": "",
    "stderr_lines": [],
    "stdout": "NAME: prometheus\nLAST DEPLOYED: Wed Mar 12 18:23:18 2025\nNAMESPACE: monitoring\nSTATUS: deployed\nREVISION: 1\nNOTES:\nkube-prometheus-stack has been installed. Check its status by running:\n  kubectl --namespace monitoring get pods -l \"release=prometheus\"\n\nGet Grafana 'admin' user password by running:\n\n  kubectl --namespace monitoring get secrets prometheus-grafana -o jsonpath=\"{.data.admin-password}\" | base64 -d ; echo\n\nAccess Grafana local instance:\n\n  export POD_NAME=$(kubectl --namespace monitoring get pod -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=prometheus\" -oname)\n  kubectl --namespace monitoring port-forward $POD_NAME 3000\n\nVisit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.",
    "stdout_lines": [
        "NAME: prometheus",
        "LAST DEPLOYED: Wed Mar 12 18:23:18 2025",
        "NAMESPACE: monitoring",
        "STATUS: deployed",
        "REVISION: 1",
        "NOTES:",
        "kube-prometheus-stack has been installed. Check its status by running:",
        "  kubectl --namespace monitoring get pods -l \"release=prometheus\"",
        "",
        "Get Grafana 'admin' user password by running:",
        "",
        "  kubectl --namespace monitoring get secrets prometheus-grafana -o jsonpath=\"{.data.admin-password}\" | base64 -d ; echo",
        "",
        "Access Grafana local instance:",
        "",
        "  export POD_NAME=$(kubectl --namespace monitoring get pod -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=prometheus\" -oname)",
        "  kubectl --namespace monitoring port-forward $POD_NAME 3000",
        "",
        "Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator."
    ]
}

TASK [energymon : Create monitoring Namespace] *********************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/prometheus-grafana.yaml:7
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_prometheus_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [energymon : Install helm prometheus-community repo] **********************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/prometheus-grafana.yaml:13
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_prometheus_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [energymon : Clean namespace file] ****************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/prometheus-grafana.yaml:17
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865096.8241987-26533-165962258627583 `" && echo ansible-tmp-1741865096.8241987-26533-165962258627583="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865096.8241987-26533-165962258627583 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/file.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmp658plqgl TO /home/ale/.ansible/tmp/ansible-tmp-1741865096.8241987-26533-165962258627583/AnsiballZ_file.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865096.8241987-26533-165962258627583/ /home/ale/.ansible/tmp/ansible-tmp-1741865096.8241987-26533-165962258627583/AnsiballZ_file.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-jybpncpkknuznqfwitnjyczqieupynuw ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865096.8241987-26533-165962258627583/AnsiballZ_file.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865096.8241987-26533-165962258627583/ > /dev/null 2>&1 && sleep 0'
ok: [localhost] => {
    "changed": false,
    "invocation": {
        "module_args": {
            "_diff_peek": null,
            "_original_basename": null,
            "access_time": null,
            "access_time_format": "%Y%m%d%H%M.%S",
            "attributes": null,
            "follow": true,
            "force": false,
            "group": null,
            "mode": null,
            "modification_time": null,
            "modification_time_format": "%Y%m%d%H%M.%S",
            "owner": null,
            "path": "namespace.yaml",
            "recurse": false,
            "selevel": null,
            "serole": null,
            "setype": null,
            "seuser": null,
            "src": null,
            "state": "absent",
            "unsafe_writes": false
        }
    },
    "path": "namespace.yaml",
    "state": "absent"
}

TASK [energymon : Install helm prometheus-community repo] **********************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/prometheus-grafana.yaml:22
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_prometheus_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [energymon : Update helm prometheus-community repo] ***********************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/prometheus-grafana.yaml:26
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_prometheus_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [energymon : Copy grafana helm values yaml] *******************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/prometheus-grafana.yaml:30
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_prometheus_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [energymon : Waiting for ingress controller startup] **********************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/prometheus-grafana.yaml:36
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_prometheus_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [energymon : Helm prometheus release] *************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/prometheus-grafana.yaml:44
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_prometheus_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [energymon : Clean helm file] *********************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/prometheus-grafana.yaml:48
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865097.4323804-26566-246736366067289 `" && echo ansible-tmp-1741865097.4323804-26566-246736366067289="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865097.4323804-26566-246736366067289 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/file.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmp_6rb3fsn TO /home/ale/.ansible/tmp/ansible-tmp-1741865097.4323804-26566-246736366067289/AnsiballZ_file.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865097.4323804-26566-246736366067289/ /home/ale/.ansible/tmp/ansible-tmp-1741865097.4323804-26566-246736366067289/AnsiballZ_file.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-ghocahmpbsxvzzurjteonjefiyafqgjy ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865097.4323804-26566-246736366067289/AnsiballZ_file.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865097.4323804-26566-246736366067289/ > /dev/null 2>&1 && sleep 0'
ok: [localhost] => {
    "changed": false,
    "invocation": {
        "module_args": {
            "_diff_peek": null,
            "_original_basename": null,
            "access_time": null,
            "access_time_format": "%Y%m%d%H%M.%S",
            "attributes": null,
            "follow": true,
            "force": false,
            "group": null,
            "mode": null,
            "modification_time": null,
            "modification_time_format": "%Y%m%d%H%M.%S",
            "owner": null,
            "path": "values.yaml",
            "recurse": false,
            "selevel": null,
            "serole": null,
            "setype": null,
            "seuser": null,
            "src": null,
            "state": "absent",
            "unsafe_writes": false
        }
    },
    "path": "values.yaml",
    "state": "absent"
}

TASK [energymon : Patch prometheus resource to update podMonitorSelector and serviceMonitorSelector] ***
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/prometheus-grafana.yaml:53
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_prometheus_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [energymon : Check if kepler is already installed] ************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/kepler.yaml:1
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865097.9814687-26595-26358230483798 `" && echo ansible-tmp-1741865097.9814687-26595-26358230483798="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865097.9814687-26595-26358230483798 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmpl48o5c8r TO /home/ale/.ansible/tmp/ansible-tmp-1741865097.9814687-26595-26358230483798/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865097.9814687-26595-26358230483798/ /home/ale/.ansible/tmp/ansible-tmp-1741865097.9814687-26595-26358230483798/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-umhtswxwbxanqcbupnacqrsestickhde ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865097.9814687-26595-26358230483798/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865097.9814687-26595-26358230483798/ > /dev/null 2>&1 && sleep 0'
changed: [localhost] => {
    "changed": true,
    "cmd": "helm --kubeconfig /etc/rancher/k3s/k3s.yaml status kepler -n monitoring",
    "delta": "0:00:00.132937",
    "end": "2025-03-13 12:24:58.370847",
    "invocation": {
        "module_args": {
            "_raw_params": "helm --kubeconfig /etc/rancher/k3s/k3s.yaml status kepler -n monitoring",
            "_uses_shell": true,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "",
    "rc": 0,
    "start": "2025-03-13 12:24:58.237910",
    "stderr": "",
    "stderr_lines": [],
    "stdout": "NAME: kepler\nLAST DEPLOYED: Wed Mar 12 18:23:38 2025\nNAMESPACE: monitoring\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None",
    "stdout_lines": [
        "NAME: kepler",
        "LAST DEPLOYED: Wed Mar 12 18:23:38 2025",
        "NAMESPACE: monitoring",
        "STATUS: deployed",
        "REVISION: 1",
        "TEST SUITE: None"
    ]
}

TASK [energymon : Add keplergl Helm repository] ********************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/kepler.yaml:6
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_kepler_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [energymon : Update keplergl Helm repository] *****************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/kepler.yaml:10
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_kepler_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [energymon : Install kepler] **********************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/kepler.yaml:14
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_kepler_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [energymon : Import grafana dashboard json (0/4)] *************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/kepler.yaml:18
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_kepler_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [energymon : Install kepler dashboard (1/4)] ******************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/kepler.yaml:24
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_kepler_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [energymon : Install kepler dashboard (2/4)] ******************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/kepler.yaml:29
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_kepler_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [energymon : Install kepler dashboard (3/4)] ******************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/kepler.yaml:33
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_kepler_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [energymon : Install kepler dashboard (4/4)] ******************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/kepler.yaml:38
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_kepler_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [energymon : Import liqonetwork dashboard json] ***************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/kepler.yaml:46
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_kepler_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [energymon : Install liqonetwork dashboard] *******************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/energymon/tasks/kepler.yaml:52
skipping: [localhost] => {
    "changed": false,
    "false_condition": "helm_kepler_exists_result.rc != 0",
    "skip_reason": "Conditional result was False"
}

TASK [liqo-setup : Check if liqoctl is already installed] **********************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/liqo-setup/tasks/main.yaml:1
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865098.9754572-26641-174609740579090 `" && echo ansible-tmp-1741865098.9754572-26641-174609740579090="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865098.9754572-26641-174609740579090 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmpy_kcs6kb TO /home/ale/.ansible/tmp/ansible-tmp-1741865098.9754572-26641-174609740579090/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865098.9754572-26641-174609740579090/ /home/ale/.ansible/tmp/ansible-tmp-1741865098.9754572-26641-174609740579090/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-zzdlfqihijxvdhrubainraywnrqhutoc ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865098.9754572-26641-174609740579090/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865098.9754572-26641-174609740579090/ > /dev/null 2>&1 && sleep 0'
fatal: [localhost]: FAILED! => {
    "changed": true,
    "cmd": "liqoctl -v",
    "delta": "0:00:00.004492",
    "end": "2025-03-13 12:24:59.231918",
    "invocation": {
        "module_args": {
            "_raw_params": "liqoctl -v",
            "_uses_shell": true,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "non-zero return code",
    "rc": 127,
    "start": "2025-03-13 12:24:59.227426",
    "stderr": "/bin/sh: 1: liqoctl: not found",
    "stderr_lines": [
        "/bin/sh: 1: liqoctl: not found"
    ],
    "stdout": "",
    "stdout_lines": []
}
...ignoring

TASK [liqo-setup : Download and install liqoctl] *******************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/liqo-setup/tasks/main.yaml:6
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865099.3231497-26669-151728238805863 `" && echo ansible-tmp-1741865099.3231497-26669-151728238805863="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865099.3231497-26669-151728238805863 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmpgncjj91d TO /home/ale/.ansible/tmp/ansible-tmp-1741865099.3231497-26669-151728238805863/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865099.3231497-26669-151728238805863/ /home/ale/.ansible/tmp/ansible-tmp-1741865099.3231497-26669-151728238805863/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-cwjmtaddqlcpjjbhedbxngfpbznljvcc ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865099.3231497-26669-151728238805863/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865099.3231497-26669-151728238805863/ > /dev/null 2>&1 && sleep 0'
changed: [localhost] => {
    "changed": true,
    "cmd": "curl -LO \"https://github.com/liqotech/liqo/releases/download/v1.0.0/liqoctl-linux-amd64\"\nchmod +x liqoctl-linux-amd64\nsudo mv liqoctl-linux-amd64 /usr/local/bin/liqoctl\n",
    "delta": "0:00:08.558783",
    "end": "2025-03-13 12:25:08.125877",
    "invocation": {
        "module_args": {
            "_raw_params": "curl -LO \"https://github.com/liqotech/liqo/releases/download/v1.0.0/liqoctl-linux-amd64\"\nchmod +x liqoctl-linux-amd64\nsudo mv liqoctl-linux-amd64 /usr/local/bin/liqoctl\n",
            "_uses_shell": true,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "",
    "rc": 0,
    "start": "2025-03-13 12:24:59.567094",
    "stderr": "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n\r  0 73.4M    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\r  9 73.4M    9 7052k    0     0  3389k      0  0:00:22  0:00:02  0:00:20 9042k\r 22 73.4M   22 16.6M    0     0  5536k      0  0:00:13  0:00:03  0:00:10 9578k\r 36 73.4M   36 27.1M    0     0  6809k      0  0:00:11  0:00:04  0:00:07 9997k\r 51 73.4M   51 37.5M    0     0  7558k      0  0:00:09  0:00:05  0:00:04  9.9M\r 65 73.4M   65 48.0M    0     0  8084k      0  0:00:09  0:00:06  0:00:03 10.0M\r 79 73.4M   79 58.4M    0     0  8447k      0  0:00:08  0:00:07  0:00:01 10.3M\r 93 73.4M   93 68.8M    0     0  8730k      0  0:00:08  0:00:08 --:--:-- 10.4M\r100 73.4M  100 73.4M    0     0  8828k      0  0:00:08  0:00:08 --:--:-- 10.4M",
    "stderr_lines": [
        "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current",
        "                                 Dload  Upload   Total   Spent    Left  Speed",
        "",
        "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0",
        "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0",
        "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0",
        "",
        "  0 73.4M    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0",
        "  9 73.4M    9 7052k    0     0  3389k      0  0:00:22  0:00:02  0:00:20 9042k",
        " 22 73.4M   22 16.6M    0     0  5536k      0  0:00:13  0:00:03  0:00:10 9578k",
        " 36 73.4M   36 27.1M    0     0  6809k      0  0:00:11  0:00:04  0:00:07 9997k",
        " 51 73.4M   51 37.5M    0     0  7558k      0  0:00:09  0:00:05  0:00:04  9.9M",
        " 65 73.4M   65 48.0M    0     0  8084k      0  0:00:09  0:00:06  0:00:03 10.0M",
        " 79 73.4M   79 58.4M    0     0  8447k      0  0:00:08  0:00:07  0:00:01 10.3M",
        " 93 73.4M   93 68.8M    0     0  8730k      0  0:00:08  0:00:08 --:--:-- 10.4M",
        "100 73.4M  100 73.4M    0     0  8828k      0  0:00:08  0:00:08 --:--:-- 10.4M"
    ],
    "stdout": "",
    "stdout_lines": []
}

TASK [liqo-setup : Install liqoctl] ********************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/liqo-setup/tasks/main.yaml:14
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865108.2197273-26714-237342978875901 `" && echo ansible-tmp-1741865108.2197273-26714-237342978875901="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865108.2197273-26714-237342978875901 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmpovukq5gv TO /home/ale/.ansible/tmp/ansible-tmp-1741865108.2197273-26714-237342978875901/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865108.2197273-26714-237342978875901/ /home/ale/.ansible/tmp/ansible-tmp-1741865108.2197273-26714-237342978875901/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-sggbbfonloyrbpmpufspghzhwvtunncr ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865108.2197273-26714-237342978875901/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865108.2197273-26714-237342978875901/ > /dev/null 2>&1 && sleep 0'
changed: [localhost] => {
    "changed": true,
    "cmd": "sudo install -o root -g root -m 0755 liqoctl /usr/local/bin/liqoctl",
    "delta": "0:00:00.369566",
    "end": "2025-03-13 12:25:08.825786",
    "invocation": {
        "module_args": {
            "_raw_params": "sudo install -o root -g root -m 0755 liqoctl /usr/local/bin/liqoctl",
            "_uses_shell": true,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "",
    "rc": 0,
    "start": "2025-03-13 12:25:08.456220",
    "stderr": "",
    "stderr_lines": [],
    "stdout": "",
    "stdout_lines": []
}

TASK [liqo-setup : Check if liqo is already installed] *************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/liqo-setup/tasks/main.yaml:18
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865108.915361-26746-255588551317029 `" && echo ansible-tmp-1741865108.915361-26746-255588551317029="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865108.915361-26746-255588551317029 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmplnabtedc TO /home/ale/.ansible/tmp/ansible-tmp-1741865108.915361-26746-255588551317029/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865108.915361-26746-255588551317029/ /home/ale/.ansible/tmp/ansible-tmp-1741865108.915361-26746-255588551317029/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-yljoayrjlprnwmbhyffxiysmtlztsdas ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865108.915361-26746-255588551317029/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865108.915361-26746-255588551317029/ > /dev/null 2>&1 && sleep 0'
fatal: [localhost]: FAILED! => {
    "changed": true,
    "cmd": "liqoctl version",
    "delta": "0:00:00.159759",
    "end": "2025-03-13 12:25:09.368336",
    "invocation": {
        "module_args": {
            "_raw_params": "liqoctl version",
            "_uses_shell": true,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "non-zero return code",
    "rc": 1,
    "start": "2025-03-13 12:25:09.208577",
    "stderr": "WARN: Failed to retrieve Liqo server version: Error from server (NotFound): deployments.apps \"liqo-controller-manager\" not found\nWARN: Is Liqo installed in your cluster? Is the cluster reachable? Do you have the permissions to access the target cluster?",
    "stderr_lines": [
        "WARN: Failed to retrieve Liqo server version: Error from server (NotFound): deployments.apps \"liqo-controller-manager\" not found",
        "WARN: Is Liqo installed in your cluster? Is the cluster reachable? Do you have the permissions to access the target cluster?"
    ],
    "stdout": "Client version: v1.0.0\nServer version: Unknown",
    "stdout_lines": [
        "Client version: v1.0.0",
        "Server version: Unknown"
    ]
}
...ignoring

TASK [liqo-setup : Install Liqo] ***********************************************
task path: /home/ale/Desktop/edge-infrastructure-ansible/playbook/roles/liqo-setup/tasks/main.yaml:23
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: ale
<127.0.0.1> EXEC /bin/sh -c 'echo ~ale && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/ale/.ansible/tmp `"&& mkdir "` echo /home/ale/.ansible/tmp/ansible-tmp-1741865109.4922814-26783-157155151311552 `" && echo ansible-tmp-1741865109.4922814-26783-157155151311552="` echo /home/ale/.ansible/tmp/ansible-tmp-1741865109.4922814-26783-157155151311552 `" ) && sleep 0'
Using module file /usr/lib/python3/dist-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/ale/.ansible/tmp/ansible-local-24330do70qxu7/tmptfre42lj TO /home/ale/.ansible/tmp/ansible-tmp-1741865109.4922814-26783-157155151311552/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/ale/.ansible/tmp/ansible-tmp-1741865109.4922814-26783-157155151311552/ /home/ale/.ansible/tmp/ansible-tmp-1741865109.4922814-26783-157155151311552/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'sudo -H -S -n  -u root /bin/sh -c '"'"'echo BECOME-SUCCESS-cqxhaeajuhygciwhdeikhxbdgjoathkf ; KUBECONFIG=/etc/rancher/k3s/k3s.yaml /usr/bin/python3 /home/ale/.ansible/tmp/ansible-tmp-1741865109.4922814-26783-157155151311552/AnsiballZ_command.py'"'"' && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/ale/.ansible/tmp/ansible-tmp-1741865109.4922814-26783-157155151311552/ > /dev/null 2>&1 && sleep 0'
fatal: [localhost]: FAILED! => {
    "changed": true,
    "cmd": "sudo liqoctl install k3s --kubeconfig=/etc/rancher/k3s/k3s.yaml --enable-metrics --set controllerManager.metrics.serviceMonitor.enabled=true --set metrics.prometheusOperator.enabled=true",
    "delta": "0:00:05.671661",
    "end": "2025-03-13 12:25:15.490718",
    "invocation": {
        "module_args": {
            "_raw_params": "sudo liqoctl install k3s --kubeconfig=/etc/rancher/k3s/k3s.yaml --enable-metrics --set controllerManager.metrics.serviceMonitor.enabled=true --set metrics.prometheusOperator.enabled=true",
            "_uses_shell": true,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "expand_argument_vars": true,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "non-zero return code",
    "rc": 1,
    "start": "2025-03-13 12:25:09.819057",
    "stderr": "\r                                                                                \rInitializing installer\n\r                                                                                \rINFO: Installer initialized\n\r                                                                                \rRetrieving cluster configuration\n\r                                                                                \rINFO: No cluster id specified. Generated: \"muddy-morning\"\r                                                                                \rINFO: Cluster configuration correctly retrieved\n\r                                                                                \rGenerating installation parameters\n\r                                                                                \rINFO: Installation parameters correctly generated\n\r                                                                                \rInstalling or upgrading Liqo... (this may take a few minutes)\n\r                                                                                \rERRO: Error installing or upgrading Liqo: create: failed to create: secrets \"sh.helm.release.v1.liqo.v1\" is forbidden: unable to create new content in namespace liqo because it is being terminated",
    "stderr_lines": [
        "",
        "                                                                                ",
        "Initializing installer",
        "",
        "                                                                                ",
        "INFO: Installer initialized",
        "",
        "                                                                                ",
        "Retrieving cluster configuration",
        "",
        "                                                                                ",
        "INFO: No cluster id specified. Generated: \"muddy-morning\"",
        "                                                                                ",
        "INFO: Cluster configuration correctly retrieved",
        "",
        "                                                                                ",
        "Generating installation parameters",
        "",
        "                                                                                ",
        "INFO: Installation parameters correctly generated",
        "",
        "                                                                                ",
        "Installing or upgrading Liqo... (this may take a few minutes)",
        "",
        "                                                                                ",
        "ERRO: Error installing or upgrading Liqo: create: failed to create: secrets \"sh.helm.release.v1.liqo.v1\" is forbidden: unable to create new content in namespace liqo because it is being terminated"
    ],
    "stdout": "",
    "stdout_lines": []
}

PLAY RECAP *********************************************************************
localhost                  : ok=24   changed=13   unreachable=0    failed=1    skipped=26   rescued=0    ignored=6   

